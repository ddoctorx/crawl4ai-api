# Crawl4AI API

<div align="center">

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.110.0-009688.svg?logo=fastapi)](https://fastapi.tiangolo.com)
[![Crawl4AI](https://img.shields.io/badge/Crawl4AI-0.6.2-orange)](https://github.com/unclecode/crawl4ai)

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

</div>

## English

A high-performance, production-ready RESTful API service built on [Crawl4AI](https://github.com/unclecode/crawl4ai), providing powerful web scraping and data extraction capabilities with modern async architecture.

### âœ¨ Features

- ğŸš€ **High Performance**: Async architecture with connection pooling for maximum throughput
- ğŸ”§ **Multiple Crawling Modes**: Single URL, batch processing, and deep website crawling
- ğŸ¯ **Smart Extraction**: CSS selectors and LLM-powered intelligent data extraction
- ğŸ›¡ï¸ **Production Ready**: Rate limiting, authentication, comprehensive error handling
- ğŸ“Š **Monitoring**: Built-in health checks, metrics, and logging
- ğŸ³ **Docker Support**: Easy deployment with Docker and docker-compose
- ğŸ“š **Full Documentation**: Interactive API docs with Swagger UI

### ğŸš€ Quick Start

#### Prerequisites

- Python 3.10+
- pip or [uv](https://github.com/astral-sh/uv) (recommended)

#### Installation

1. **Clone the repository**

```bash
git clone https://github.com/yourusername/crawl4ai-api.git
cd crawl4ai-api
```

2. **Install dependencies**

```bash
# Using the provided script (recommended)
./run.sh --install-only

# Or manually
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
playwright install chromium
```

3. **Configure environment** (optional)

```bash
cp .env.example .env
# Edit .env with your settings
```

4. **Run the service**

```bash
# Using the script
./run.sh

# Or manually
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```

The API will be available at `http://localhost:8001`

### ğŸ“– API Documentation

Once running, access the interactive documentation:

- **Swagger UI**: http://localhost:8001/docs
- **ReDoc**: http://localhost:8001/redoc

### ğŸ”Œ API Endpoints

#### Core Endpoints

| Method | Endpoint            | Description            |
| ------ | ------------------- | ---------------------- |
| POST   | `/api/crawl/url`    | Crawl a single URL     |
| POST   | `/api/crawl/batch`  | Crawl multiple URLs    |
| POST   | `/api/crawl/deep`   | Deep crawl a website   |
| POST   | `/api/extract/llm`  | Extract data using LLM |
| GET    | `/api/crawl/health` | Health check           |
| GET    | `/api/version`      | API version info       |

### ğŸ’¡ Usage Examples

#### Python

```python
import requests

# Crawl a single URL
response = requests.post(
    "http://127.0.0.1:8001/api/crawl/url",
    json={
        "url": "https://www.anthropic.com/engineering/building-effective-agents",
        "js_enabled": True,
        "bypass_cache": True
    }
)
result = response.json()
print(result["markdown"])
```

#### JavaScript

```javascript
// Crawl with CSS extraction
fetch('http://127.0.0.1:8001/api/crawl/url', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({
    url: 'https://www.anthropic.com/engineering/building-effective-agents',
    js_enabled: true,
    bypass_cache: true,
  }),
})
  .then(response => {
    if (!response.ok) throw new Error(`HTTP ${response.status}`);
    return response.json(); // æ ¹æ®æ¥å£è¿”å›ç±»å‹è°ƒæ•´
  })
  .then(data => {
    console.log('æŠ“å–ç»“æœ:', data);
  })
  .catch(error => {
    console.error('è¯·æ±‚å¤±è´¥:', error);
  });
```

#### cURL

```bash
curl -X POST http://127.0.0.1:8001/api/crawl/url \
  -H "Content-Type: application/json" \
  -d '{
    "url": "https://www.anthropic.com/engineering/building-effective-agents",
    "js_enabled": true,
    "bypass_cache": true
  }'
```

```bash
# Deep crawl a website
curl -X POST http://127.0.0.1:8001/api/crawl/deep \
  -H "Content-Type: application/json" \
  -d '{
    "start_url": "https://www.anthropic.com/engineering/building-effective-agents",
    "max_depth": 2,
    "max_pages": 10,
    "include_patterns": ["*/blog/*"],
    "exclude_patterns": ["*/admin/*"]
  }'
```

### ğŸ³ Docker Deployment

#### Using Docker

```bash
# Build the image
docker build -t crawl4ai-api .

# Run the container
docker run -d \
  -p 8001:8001 \
  -e API_KEY_ENABLED=true \
  -e API_KEYS=your-secret-key \
  --name crawl4ai-api \
  crawl4ai-api
```

#### Using Docker Compose

```yaml
version: '3.10'

services:
  api:
    build: .
    ports:
      - '8001:8001'
    environment:
      - API_KEY_ENABLED=true
      - API_KEYS=${API_KEYS}
      - RATE_LIMIT_CALLS=100
      - RATE_LIMIT_PERIOD=60
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped
```

### ğŸ”§ Configuration

The service can be configured via environment variables or `.env` file:

| Variable                | Description                            | Default |
| ----------------------- | -------------------------------------- | ------- |
| `PORT`                  | API service port                       | `8001`  |
| `API_KEY_ENABLED`       | Enable API key authentication          | `false` |
| `API_KEYS`              | Comma-separated list of valid API keys | `[]`    |
| `RATE_LIMIT_ENABLED`    | Enable rate limiting                   | `true`  |
| `RATE_LIMIT_CALLS`      | Max requests per period                | `100`   |
| `RATE_LIMIT_PERIOD`     | Rate limit time window (seconds)       | `60`    |
| `BROWSER_HEADLESS`      | Run browser in headless mode           | `true`  |
| `MAX_CONCURRENT_CRAWLS` | Max concurrent crawl operations        | `5`     |
| `LOG_LEVEL`             | Logging level                          | `INFO`  |

See `.env.example` for all available options.

### ğŸ›¡ï¸ Security

#### API Key Authentication

Enable API key authentication for production:

```bash
API_KEY_ENABLED=true
API_KEYS=key1,key2,key3
```

Then include the key in requests:

```bash
curl -H "Authorization: Bearer your-api-key" http://localhost:8001/api/crawl/url
```

#### Rate Limiting

Requests are rate-limited by default. Configure limits via:

```bash
RATE_LIMIT_CALLS=100  # requests
RATE_LIMIT_PERIOD=60  # seconds
```

#### Health Checks

```bash
curl http://localhost:8001/api/crawl/health
```

### ğŸ“‹ Roadmap

- [ ] WebSocket support for real-time crawling
- [ ] Redis integration for distributed caching
- [ ] PostgreSQL storage backend
- [ ] Advanced scheduling system
- [ ] Browser session management
- [ ] Webhook notifications
- [ ] GraphQL API endpoint
- [ ] Enhanced LLM extraction strategies

### ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### ğŸ™ Acknowledgments

- [Crawl4AI](https://github.com/unclecode/crawl4ai) - The powerful crawling library this API is built on
- [FastAPI](https://fastapi.tiangolo.com/) - The modern web framework
- [Playwright](https://playwright.dev/) - Browser automation

---

## ä¸­æ–‡

åŸºäº [Crawl4AI](https://github.com/unclecode/crawl4ai) æ„å»ºçš„é«˜æ€§èƒ½ã€ç”Ÿäº§å°±ç»ªçš„ RESTful API æœåŠ¡ï¼Œæä¾›å¼ºå¤§çš„ç½‘é¡µçˆ¬å–å’Œæ•°æ®æå–åŠŸèƒ½ï¼Œé‡‡ç”¨ç°ä»£å¼‚æ­¥æ¶æ„ã€‚

### âœ¨ ç‰¹æ€§

- ğŸš€ **é«˜æ€§èƒ½**: å¼‚æ­¥æ¶æ„å’Œè¿æ¥æ± ï¼Œå®ç°æœ€å¤§ååé‡
- ğŸ”§ **å¤šç§çˆ¬å–æ¨¡å¼**: å•ä¸ª URLã€æ‰¹é‡å¤„ç†å’Œæ·±åº¦ç½‘ç«™çˆ¬å–
- ğŸ¯ **æ™ºèƒ½æå–**: CSS é€‰æ‹©å™¨å’ŒåŸºäº LLM çš„æ™ºèƒ½æ•°æ®æå–
- ğŸ›¡ï¸ **ç”Ÿäº§å°±ç»ª**: é€Ÿç‡é™åˆ¶ã€èº«ä»½éªŒè¯ã€å…¨é¢çš„é”™è¯¯å¤„ç†
- ğŸ“Š **ç›‘æ§æ”¯æŒ**: å†…ç½®å¥åº·æ£€æŸ¥ã€æŒ‡æ ‡å’Œæ—¥å¿—è®°å½•
- ğŸ³ **Docker æ”¯æŒ**: ä½¿ç”¨ Docker å’Œ docker-compose è½»æ¾éƒ¨ç½²
- ğŸ“š **å®Œæ•´æ–‡æ¡£**: ä½¿ç”¨ Swagger UI çš„äº¤äº’å¼ API æ–‡æ¡£

### ğŸš€ å¿«é€Ÿå¼€å§‹

è¯¦ç»†çš„å®‰è£…å’Œä½¿ç”¨è¯´æ˜è¯·å‚è€ƒä¸Šæ–¹çš„è‹±æ–‡æ–‡æ¡£ã€‚

### ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦æƒ…è¯·è§ [LICENSE](LICENSE) æ–‡ä»¶ã€‚
